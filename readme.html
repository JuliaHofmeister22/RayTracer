<meta charset="utf-8" lang="en">  <!-- this line allows UTF-8 characters in the .html file -->

                    **P04_Raytrace**


Author
=============
<div class="noheader firstcol">
                  |             
------------------|-------------
names             | Rose White, Julia Hofmeister, Courtney Masters
computer + OS     | Windows Latitude 5401, MacBook Pro BigSur, MacBook Pro 
time to complete  | 65 hours
additional help   | Dr. Denning
</div>




Example Section
===================

Reference Images
---------------------

![Our video, non mpeg version](ffmpegSnowMovie.mp4 width="200px")

![Beginning Scene](images/snowPNG/snow_particles000.png width="200px") ![Middle Scene](images/snowPNG/snow_particles008.png width="200px") ![End Scene](images/snowPNG/snow_particles022.png width="200px")

![Snow Flurry Beginning Scene](images/snowPNG/snow_xflurry001.png width="200px") ![Snow Flurry Middle Scene](images/snowPNG/snow_xflurry009.png width="200px") ![Snow Flurry End Scene](images/snowPNG/snow_xflurry017.png width="200px")

![Ice Texture](iceTexture.jpeg width="200px")

<iframe width="200"
src="https://youtu.be/xEeBQsR5W-o">
</iframe>
If mpeg video does not show, go to: https://youtu.be/xEeBQsR5W-o

Julia's Feature - Procedural Motion
-------------------------
I implemented the visual effect of the falling snow in this video.  Using Blender, I used physics and altered the gravity for particles falling from a plane to get the effect of falling snow.
  Each particle is rendered as an icosphere object and is colored completely white.  They are all scaled to .05, but some appear larger than others due to their proximity to the camera in the scene.
For lighting, I initially chose to use a point light, but the shadows that it created did not give the effect that I wanted, so I changed the main light to be a spot light.  

To export these particles to a scene was a bit of a challenge.  Dr. Denning helped me debug some the Depsgraph issues I was having and helped me alter his initial exporter code to be able to handle particles.  
I exported 24 scenes of falling snow for this project, each one 5 frames apart from each other in my Snow_Particles.blend file, at 192x108 resolution.  I later changed this resolution to 360x360 so that the 
images were a bit clearer, but also didn’t take an extremely long time to render.  

This feature is important because it is the animation of our video, and also because it is a feature that can be used to make some very cool effects.  Other ways that we considered using this feature was 
to implement different season - perhaps making rain fall hard or changing the way the snow fell to make it look like the scene was very windy. Outside of this project, I can see this feature being used in 
a variety of situations (pieces of a building falling after an explosion, for example).

I’m not sure what limitations this feature has.  I think depending on what you want to do, there is probably a way to do it in Blender, or at least a way to fake it.  
For me, the only limitation that I ran into was my own lack of knowledge in how to use Blender.

A couple things that I learned from working on this feature are very closely related to working with Blender. Not having used Blender before, I spent quite a bit of time learning how to navigate the tools 
and figuring out how to make the particles fall how I wanted them to.  Navigating light intensity was also something that I learned from this feature - I needed my light to be bright enough to capture the 
particles in the scene, but not too bright that it washed out the color of the ice sphere in scenes.  

As mentioned above, a few issues that I ran into were related to the exporter, which Dr. Denning helped me fix.  I also struggled with scaling the particles to the correct size - I had scaled the particles 
themselves instead of the icosphere they were imitating.  I believe I spent about 6 hours researching Blender and creating the scenes,
about 8 hours debugging the exporter and getting my light/camera information to be correct, and then another 4 hours rendering
the original scenes to make sure that they were correct, for a total of 18 hours.  (I also spent another 12 hours rendering these scenes at a higher
resolution for the final video)


Courtney's Feature - Texture Mapping
-------------------------

To implement texture mapping, I added an optional texture attribute to each shape that holds a ppm image containing the desired texture. I then wrote a
helper function that maps each point from a shape to a pixel in the image and returns color from the image for the shape.
It returns an RGBColor that can be used in the irradience function if a texture is provided. Although this part of the project is pretty straightforward, I did have some
trouble understanding the math and needed to get help from Dr. Denning. 

The majority of my time was spent reading through slides and trying to figure out what part of the raytracer needed to
be modified. It took me a while to actually understand that I needed to map pixels from an image to a shape. I always feel a little
hesitant to modeify existing classes but I knew that I had to add a texture attribute to have access to a ppm image to get texture from.
Even after that, the math felt pretty complicated to me and I had to get some guidence from Dr. Denning with the specific syntax for the theta
and phi. After I finished, I felt like I understood everything pretty well, and I think I could easily redo it if need be.
I did not really discuss with anyone else on this. Like I mentioned, the majority of my time was spent reading online resources and looking
through slides. I wasn't sure I was even close to the right track, and to be honest, I got pretty lucky that I started in the right direction
because I was unsure of what I was doing for most of the first steps.

Rose's Feature - Video Encoder
-------------------------
The video encoder takes the raytracter's rendered images and stores them in an mpeg video. This was accomplished by taking working C++ code that already performed this feature, and translating it into dart. Dr. Denning also provided his own changes to the code to make the "code translation" process easier.

Syntax error bugs were the most troublesome. Fortunately if there are any bugs in the code, it is REALLY obvious. My video had read some bits and bytes wrong and was beginning to output skewed images. Another time, my video would just start and stop really fast. These were clear signs that something was wrong, and these implementations were some of my bigger bugs.

As mentioned above, I'm fairly confident no bugs remain since the video converter would give obvious signals that something was wrong if it was. Either the image would appear out of place, distorted, or would just not play. Since the algorithms were already written for me, the bugs were a cause of my incorrect implementation, either of dart syntax or bit/byte factors.


Reflection: I recieve many hours of help from Dr. Denning, and also was able to bounce ideas off my teammates. I also got onto several coding websites to try and understand dart better. The biggest problems I encountered were syntax issues, code placement, and not being able to make the video play (another code placement issue).
I probablt spent 2 hours writing code, and 20-25 hours debugging it. Another few hours were spent on correctly using Git so that I didn't delete anything again...

This project taught me to be a better debugger, and made me more familiar with dart. This project also built character because it took a lot of patience to work through. I think the idea of a video converter is super cool, but this particular elective definitely took longer and was more difficult to complete than expected. If I could go back, I'd probably choose to do another elective so I wouldn't have held my team back for so long, but I also appreciate the creativity in tasks we were able to choose from.


Creative Artifact (Julia) - *cue Elsa singing ‘Let it Go’* 
-------------------------
I used what I learned from my procedural motion feature and designed a creative artifact for this project by simulating a “snow flurry”.  The scenes for this section of the video were created by using
 a Bezier curve in blender, and then activating a force field (type is curve guide) to pull particles along the curve.  The particles are again rendered as an icosphere object and scaled to .05. 

I exported 21 scenes to show this artifact at the end of our video.  Each scene was exported using the same exporter that I used for the falling snow scenes, and they are 2-3 frames apart from each
 other in my snow_flurry.blend file and are currently being rendered at 360x360.

*note: I had some issues with the camera view for this file.  For some reason, what I was seeing from the view is not the same as what would render for the same timestamp.  I ended up having to just move
 the camera around enough so that what I rendered would show the flurry as I wanted it to, but I still do not understand why the camera was not showing me the view that should have rendered.
I probably spent about 6 hours working on this artifact - mostly trying to fix my camera issues and learning how to use a curve force field.

Extra ffmpeg Video (Julia) - 1 hour
-------------------------
See .mp4 video above




<!--   Feel free to modify the following to fit a theme of your choosing   -->
<link href="https://fonts.googleapis.com/css?family=Open+Sans&display=swap" rel="stylesheet"> <!-- a sans-serif font -->
<style>  /* A TAYLOR-INSPIRED THEME */
    body {font-family:'Open Sans',sans-serif;}
    .md a:link, .md a:visited {color:hsl(252,23.0%,44.3%); font-family:'Open Sans',sans-serif;}
    .md table.table th {background-color:hsl(252,23.0%,44.3%);}
    .md .noheader th {display:none;}
    .md .firstcol td:first-child {white-space:pre;color:white;vertical-align:top;font-weight:bold;border-color:black;background:hsl(252,23.0%,54.3%);}
    .md .firstcol tr:nth-child(even) td:first-child {background:hsl(252,23.0%,44.3%);}
</style>


<!-- ****************************** -->
<!--    Leave the content below     -->
<!-- ****************************** -->

<!-- The script and style below are added for clarity and to workaround a bug -->
<script>
    // this is a hack to workaround a bug in Markdeep+Mathjax, where
    // `&#36;`` is automatically converted to `\(`` and `\)`` too soon.
    // the following code will replace the innerHTML of all elements
    // with class "dollar" with a dollar sign.
    setTimeout(function() {
        var dollars = document.getElementsByClassName('dollar');
        for(var i = 0; i < dollars.length; i++) {
            dollars[i].innerHTML = '&#' + '36;'; // split to prevent conversion to $
        }
    }, 1000);
</script>
<style>
    /* adding some styling to <code> tags (but not <pre><code> coding blocks!) */
    :not(pre) > code {
        background-color: rgba(0,0,0,0.05);
        outline: 1px solid rgba(0,0,0,0.15);
        margin-left: 0.25em;
        margin-right: 0.25em;
    }
    /* fixes table of contents of medium-length document from looking weird if admonitions are behind */
    .md div.mediumTOC { background: white; }
    .md div.admonition { position: initial !important; }
</style>

<!--   Leave the following Markdeep formatting code, as this will format your text above to look nice in a wed browser   -->
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script>
<script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible");</script>
